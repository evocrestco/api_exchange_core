"""
End-to-End Test Harness for API Exchange Core Framework.

This module provides the main test harness functionality for running
comprehensive end-to-end tests of the framework with self-verification.
"""

import time
from typing import Any, Dict, List, Optional
from pathlib import Path

from src.context.tenant_context import tenant_context
from src.processing.processing_service import ProcessingService
from src.processing.duplicate_detection import DuplicateDetectionService
from src.processing.entity_attributes import EntityAttributeBuilder
from src.processing.processor_config import ProcessorConfig
from src.services.entity_service import EntityService
from src.services.state_tracking_service import StateTrackingService
from src.services.processing_error_service import ProcessingErrorService
from src.repositories.entity_repository import EntityRepository
from src.db.db_config import DatabaseConfig, DatabaseManager
from src.processors.processor_handler import ProcessorHandler
from src.utils.logger import get_logger
from tests.harness.processors.verification_processor import VerificationProcessor
from tests.harness.processors.validation_processor import ValidationProcessor
from tests.harness.utils.test_data_generators import TestMessageGenerator
from tests.harness.utils.results_collector import ResultsCollector, TestRunSummary


class TestHarness:
    """
    Main test harness for end-to-end framework testing.
    
    This class orchestrates the execution of test scenarios using the
    VerificationProcessor and ValidationProcessor to validate that
    the framework is working correctly.
    """
    
    def __init__(
        self,
        db_config: DatabaseConfig,
        results_storage_path: Optional[Path] = None,
        enable_state_tracking: bool = True,
        enable_error_service: bool = True,
    ):
        """
        Initialize the test harness.
        
        Args:
            db_config: Database configuration for testing
            results_storage_path: Optional path to store test results
            enable_state_tracking: Whether to enable state tracking service
            enable_error_service: Whether to enable error service
        """
        self.db_config = db_config
        self.results_storage_path = results_storage_path
        self.logger = get_logger()
        
        # Initialize framework services
        self.db_manager = DatabaseManager(db_config)
        self.entity_repository = EntityRepository(db_manager=self.db_manager)
        self.entity_service = EntityService(entity_repository=self.entity_repository)
        self.duplicate_detection_service = DuplicateDetectionService(
            entity_repository=self.entity_repository
        )
        self.attribute_builder = EntityAttributeBuilder()
        self.processing_service = ProcessingService(
            entity_service=self.entity_service,
            entity_repository=self.entity_repository,
            duplicate_detection_service=self.duplicate_detection_service,
            attribute_builder=self.attribute_builder,
        )
        
        # Optional services
        self.state_tracking_service = None
        if enable_state_tracking:
            # Initialize state tracking service if available
            try:
                from src.repositories.state_transition_repository import StateTransitionRepository
                state_repo = StateTransitionRepository(db_manager=self.db_manager)
                self.state_tracking_service = StateTrackingService(repository=state_repo)
            except ImportError:
                self.logger.warning(\"StateTrackingService not available\")\n        \n        self.processing_error_service = None\n        if enable_error_service:\n            # Initialize error service if available\n            try:\n                from src.repositories.processing_error_repository import ProcessingErrorRepository\n                error_repo = ProcessingErrorRepository(db_manager=self.db_manager)\n                self.processing_error_service = ProcessingErrorService(repository=error_repo)\n            except ImportError:\n                self.logger.warning(\"ProcessingErrorService not available\")\n        \n        # Create processor handlers\n        self._create_processor_handlers()\n    \n    def _create_processor_handlers(self) -> None:\n        \"\"\"Create processor handlers for verification and validation.\"\"\"\n        # Configuration for verification processor\n        verification_config = ProcessorConfig(\n            processor_name=\"verification_processor\",\n            processor_version=\"1.0.0\",\n            enable_state_tracking=True,\n            is_source_processor=True,\n            enable_duplicate_detection=True,\n            processing_stage=\"verification\",\n        )\n        \n        # Create verification processor\n        verification_processor = VerificationProcessor(\n            entity_service=self.entity_service,\n            processing_service=self.processing_service,\n            config=verification_config,\n            state_tracking_service=self.state_tracking_service,\n            processing_error_service=self.processing_error_service,\n        )\n        \n        # Create verification handler\n        self.verification_handler = ProcessorHandler(\n            processor=verification_processor,\n            config=verification_config,\n            processing_service=self.processing_service,\n            state_tracking_service=self.state_tracking_service,\n            error_service=self.processing_error_service,\n        )\n        \n        # Configuration for validation processor\n        validation_config = ProcessorConfig(\n            processor_name=\"validation_processor\",\n            processor_version=\"1.0.0\",\n            enable_state_tracking=False,  # Validation is terminal\n            is_terminal_processor=True,\n            processing_stage=\"validation\",\n        )\n        \n        # Create validation processor\n        validation_processor = ValidationProcessor(config=validation_config)\n        \n        # Create validation handler\n        self.validation_handler = ProcessorHandler(\n            processor=validation_processor,\n            config=validation_config,\n            processing_service=self.processing_service,\n            state_tracking_service=self.state_tracking_service,\n            error_service=self.processing_error_service,\n        )\n    \n    def run_single_test(\n        self, \n        test_message,\n        tenant_id: Optional[str] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Run a single test scenario.\n        \n        Args:\n            test_message: Test message to execute\n            tenant_id: Optional tenant ID (extracted from message if not provided)\n            \n        Returns:\n            Test outcome dictionary\n        \"\"\"\n        tenant_id = tenant_id or test_message.entity_reference.tenant_id\n        test_id = test_message.payload.get(\"test_id\", \"unknown\")\n        \n        self.logger.info(\n            f\"Running test: {test_id}\",\n            extra={\n                \"test_id\": test_id,\n                \"test_type\": test_message.payload.get(\"test_type\", \"unknown\"),\n                \"tenant_id\": tenant_id,\n            }\n        )\n        \n        try:\n            # Step 1: Run verification processor with tenant context\n            with tenant_context(tenant_id):\n                verification_result = self.verification_handler.execute(test_message)\n            \n            if not verification_result.success:\n                return {\n                    \"test_id\": test_id,\n                    \"test_type\": test_message.payload.get(\"test_type\", \"unknown\"),\n                    \"passed\": False,\n                    \"error_message\": f\"Verification processor failed: {verification_result.error_message}\",\n                    \"processing_duration_ms\": verification_result.processing_duration_ms,\n                }\n            \n            # Step 2: Extract verification results and create validation message\n            if not verification_result.output_messages:\n                return {\n                    \"test_id\": test_id,\n                    \"test_type\": test_message.payload.get(\"test_type\", \"unknown\"),\n                    \"passed\": False,\n                    \"error_message\": \"No output message from verification processor\",\n                    \"processing_duration_ms\": verification_result.processing_duration_ms,\n                }\n            \n            validation_message = verification_result.output_messages[0]\n            \n            # Step 3: Run validation processor\n            with tenant_context(tenant_id):\n                validation_result = self.validation_handler.execute(validation_message)\n            \n            if not validation_result.success:\n                return {\n                    \"test_id\": test_id,\n                    \"test_type\": test_message.payload.get(\"test_type\", \"unknown\"),\n                    \"passed\": False,\n                    \"error_message\": f\"Validation processor failed: {validation_result.error_message}\",\n                    \"processing_duration_ms\": (\n                        verification_result.processing_duration_ms + \n                        validation_result.processing_duration_ms\n                    ),\n                }\n            \n            # Step 4: Extract final test outcome\n            test_outcome = validation_result.processing_metadata.get(\"test_outcome\", {})\n            \n            if not test_outcome:\n                return {\n                    \"test_id\": test_id,\n                    \"test_type\": test_message.payload.get(\"test_type\", \"unknown\"),\n                    \"passed\": False,\n                    \"error_message\": \"No test outcome from validation processor\",\n                    \"processing_duration_ms\": (\n                        verification_result.processing_duration_ms + \n                        validation_result.processing_duration_ms\n                    ),\n                }\n            \n            return test_outcome\n            \n        except Exception as e:\n            self.logger.error(\n                f\"Test execution failed: {test_id}\",\n                extra={\n                    \"test_id\": test_id,\n                    \"error\": str(e),\n                },\n                exc_info=True\n            )\n            \n            return {\n                \"test_id\": test_id,\n                \"test_type\": test_message.payload.get(\"test_type\", \"unknown\"),\n                \"passed\": False,\n                \"error_message\": f\"Test execution exception: {str(e)}\",\n                \"processing_duration_ms\": 0.0,\n            }\n    \n    def run_test_scenario(\n        self,\n        scenario_name: str,\n        tenant_id: str = \"test_harness\",\n        **scenario_kwargs: Any,\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Run a predefined test scenario.\n        \n        Args:\n            scenario_name: Name of the test scenario to run\n            tenant_id: Tenant ID for the test\n            **scenario_kwargs: Additional arguments for scenario generation\n            \n        Returns:\n            Test outcome dictionary\n        \"\"\"\n        # Generate test message based on scenario\n        if scenario_name == \"entity_creation\":\n            test_message = TestMessageGenerator.create_entity_creation_test(\n                tenant_id=tenant_id,\n                **scenario_kwargs\n            )\n        elif scenario_name == \"multi_stage\":\n            test_message = TestMessageGenerator.create_multi_stage_test(\n                tenant_id=tenant_id,\n                **scenario_kwargs\n            )\n        elif scenario_name == \"error_handling\":\n            test_message = TestMessageGenerator.create_error_handling_test(\n                tenant_id=tenant_id,\n                **scenario_kwargs\n            )\n        else:\n            raise ValueError(f\"Unknown test scenario: {scenario_name}\")\n        \n        return self.run_single_test(test_message, tenant_id)\n    \n    def run_test_suite(\n        self,\n        scenarios: List[Dict[str, Any]],\n        collect_results: bool = True,\n    ) -> TestRunSummary:\n        \"\"\"\n        Run a complete test suite with multiple scenarios.\n        \n        Args:\n            scenarios: List of scenario configurations\n            collect_results: Whether to collect and aggregate results\n            \n        Returns:\n            TestRunSummary with aggregated results\n        \"\"\"\n        results_collector = ResultsCollector(\n            storage_path=self.results_storage_path\n        ) if collect_results else None\n        \n        self.logger.info(\n            f\"Starting test suite with {len(scenarios)} scenarios\",\n            extra={\n                \"scenario_count\": len(scenarios),\n                \"run_id\": results_collector.run_id if results_collector else \"no-collection\",\n            }\n        )\n        \n        for i, scenario_config in enumerate(scenarios, 1):\n            scenario_name = scenario_config.pop(\"scenario\", \"entity_creation\")\n            \n            self.logger.info(\n                f\"Running scenario {i}/{len(scenarios)}: {scenario_name}\",\n                extra={\"scenario_name\": scenario_name, \"scenario_index\": i}\n            )\n            \n            try:\n                outcome = self.run_test_scenario(scenario_name, **scenario_config)\n                \n                if results_collector:\n                    results_collector.add_test_outcome(outcome)\n                \n                # Log individual test result\n                status = \"PASSED\" if outcome.get(\"passed\") else \"FAILED\"\n                self.logger.info(\n                    f\"Test {i} {status}: {outcome.get('test_id', 'unknown')}\",\n                    extra={\n                        \"test_id\": outcome.get(\"test_id\"),\n                        \"test_passed\": outcome.get(\"passed\"),\n                        \"scenario_name\": scenario_name,\n                    }\n                )\n                \n            except Exception as e:\n                self.logger.error(\n                    f\"Scenario {i} failed with exception: {e}\",\n                    extra={\"scenario_name\": scenario_name, \"error\": str(e)},\n                    exc_info=True\n                )\n                \n                if results_collector:\n                    results_collector.add_test_outcome({\n                        \"test_id\": f\"scenario-{i}-{scenario_name}\",\n                        \"test_type\": scenario_name,\n                        \"passed\": False,\n                        \"error_message\": f\"Scenario execution failed: {str(e)}\",\n                        \"processing_duration_ms\": 0.0,\n                    })\n        \n        # Finalize and return summary\n        if results_collector:\n            summary = results_collector.finalize_run()\n            \n            # Save results if storage is configured\n            if self.results_storage_path:\n                saved_file = results_collector.save_results(summary)\n                self.logger.info(\n                    f\"Test results saved to: {saved_file}\",\n                    extra={\"results_file\": str(saved_file)}\n                )\n            \n            # Log final summary\n            self.logger.info(\n                f\"Test suite completed: {summary.get_success_rate():.1f}% success rate\",\n                extra={\n                    \"run_id\": summary.run_id,\n                    \"total_tests\": summary.total_tests,\n                    \"passed_tests\": summary.passed_tests,\n                    \"failed_tests\": summary.failed_tests,\n                    \"success_rate\": summary.get_success_rate(),\n                    \"duration_seconds\": summary.run_duration_seconds,\n                }\n            )\n            \n            return summary\n        else:\n            # Return empty summary if not collecting results\n            from tests.harness.utils.results_collector import TestRunSummary\n            return TestRunSummary(\n                run_id=\"no-collection\",\n                start_time=time.time(),\n                end_time=time.time(),\n                total_tests=len(scenarios),\n                passed_tests=0,\n                failed_tests=0,\n                test_outcomes=[],\n            )\n    \n    def create_default_test_suite(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Create a default test suite with basic scenarios.\n        \n        Returns:\n            List of scenario configurations\n        \"\"\"\n        return [\n            # Basic entity creation tests\n            {\n                \"scenario\": \"entity_creation\",\n                \"tenant_id\": \"test_harness_1\",\n                \"canonical_type\": \"basic_test\",\n                \"verify_database\": True,\n                \"verify_state_tracking\": True,\n                \"verify_error_handling\": True,\n            },\n            {\n                \"scenario\": \"entity_creation\",\n                \"tenant_id\": \"test_harness_2\",\n                \"canonical_type\": \"second_test\",\n                \"verify_database\": True,\n                \"verify_state_tracking\": True,\n                \"verify_error_handling\": True,\n            },\n            # Multi-stage pipeline test\n            {\n                \"scenario\": \"multi_stage\",\n                \"tenant_id\": \"test_harness_1\",\n                \"stages\": 3,\n            },\n        ]\n    \n    def cleanup(self) -> None:\n        \"\"\"\n        Cleanup resources and close connections.\n        \"\"\"\n        if hasattr(self.db_manager, 'close'):\n            self.db_manager.close()\n        \n        self.logger.info(\"Test harness cleanup completed\")"
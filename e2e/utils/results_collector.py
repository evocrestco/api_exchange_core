"""
Results collection utilities for the test harness.

This module provides utilities to collect, aggregate, and report
test results from the validation processor and other harness components.
"""

import json
from typing import Any, Dict, List, Optional
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path


@dataclass
class TestRunSummary:
    """Summary of a complete test harness run."""
    run_id: str
    start_time: datetime
    end_time: Optional[datetime]
    total_tests: int
    passed_tests: int
    failed_tests: int
    test_outcomes: List[Dict[str, Any]]
    run_duration_seconds: Optional[float] = None
    
    def calculate_duration(self) -> None:
        """Calculate run duration if end_time is available."""
        if self.end_time:
            self.run_duration_seconds = (self.end_time - self.start_time).total_seconds()
    
    def get_success_rate(self) -> float:
        """Get success rate as percentage."""
        if self.total_tests == 0:
            return 0.0
        return (self.passed_tests / self.total_tests) * 100
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            \"run_id\": self.run_id,\n            \"start_time\": self.start_time.isoformat(),\n            \"end_time\": self.end_time.isoformat() if self.end_time else None,\n            \"total_tests\": self.total_tests,\n            \"passed_tests\": self.passed_tests,\n            \"failed_tests\": self.failed_tests,\n            \"success_rate_percent\": self.get_success_rate(),\n            \"run_duration_seconds\": self.run_duration_seconds,\n            \"test_outcomes\": self.test_outcomes,\n        }\n    \n    def get_summary_report(self) -> str:\n        \"\"\"Get human-readable summary report.\"\"\"\n        status = \"✅ PASSED\" if self.failed_tests == 0 else \"❌ FAILED\"\n        duration = f\"{self.run_duration_seconds:.1f}s\" if self.run_duration_seconds else \"unknown\"\n        \n        lines = [\n            f\"Test Run Summary: {self.run_id}\",\n            f\"Status: {status}\",\n            f\"Duration: {duration}\",\n            f\"Tests: {self.passed_tests}/{self.total_tests} passed ({self.get_success_rate():.1f}%)\",\n            \"\",\n        ]\n        \n        if self.failed_tests > 0:\n            lines.append(\"Failed Tests:\")\n            for outcome in self.test_outcomes:\n                if not outcome.get(\"passed\", False):\n                    lines.append(f\"  - {outcome.get('test_id', 'unknown')}: {outcome.get('error_message', 'unknown error')}\")\n        \n        return \"\\n\".join(lines)\n\n\nclass ResultsCollector:\n    \"\"\"\n    Collector for test harness results with aggregation and reporting capabilities.\n    \n    This class collects test outcomes from validation processors and provides\n    methods to aggregate results, generate reports, and optionally store\n    results for historical analysis.\n    \"\"\"\n    \n    def __init__(self, run_id: Optional[str] = None, storage_path: Optional[Path] = None):\n        \"\"\"\n        Initialize the results collector.\n        \n        Args:\n            run_id: Unique identifier for this test run (generated if not provided)\n            storage_path: Optional path to store results files\n        \"\"\"\n        self.run_id = run_id or f\"harness-run-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}\"\n        self.storage_path = storage_path\n        self.start_time = datetime.utcnow()\n        self.end_time: Optional[datetime] = None\n        self.test_outcomes: List[Dict[str, Any]] = []\n        self.metadata: Dict[str, Any] = {}\n    \n    def add_test_outcome(self, outcome: Dict[str, Any]) -> None:\n        \"\"\"\n        Add a test outcome to the collection.\n        \n        Args:\n            outcome: Test outcome dictionary from ValidationProcessor\n        \"\"\"\n        # Add timestamp if not present\n        if \"timestamp\" not in outcome:\n            outcome[\"timestamp\"] = datetime.utcnow().isoformat()\n        \n        self.test_outcomes.append(outcome)\n    \n    def add_metadata(self, key: str, value: Any) -> None:\n        \"\"\"\n        Add metadata to the test run.\n        \n        Args:\n            key: Metadata key\n            value: Metadata value\n        \"\"\"\n        self.metadata[key] = value\n    \n    def finalize_run(self) -> TestRunSummary:\n        \"\"\"\n        Finalize the test run and generate summary.\n        \n        Returns:\n            TestRunSummary with aggregated results\n        \"\"\"\n        self.end_time = datetime.utcnow()\n        \n        # Count results\n        total_tests = len(self.test_outcomes)\n        passed_tests = sum(1 for outcome in self.test_outcomes if outcome.get(\"passed\", False))\n        failed_tests = total_tests - passed_tests\n        \n        summary = TestRunSummary(\n            run_id=self.run_id,\n            start_time=self.start_time,\n            end_time=self.end_time,\n            total_tests=total_tests,\n            passed_tests=passed_tests,\n            failed_tests=failed_tests,\n            test_outcomes=self.test_outcomes.copy(),\n        )\n        \n        summary.calculate_duration()\n        return summary\n    \n    def get_current_summary(self) -> TestRunSummary:\n        \"\"\"\n        Get current summary without finalizing the run.\n        \n        Returns:\n            TestRunSummary with current results\n        \"\"\"\n        current_time = datetime.utcnow()\n        \n        total_tests = len(self.test_outcomes)\n        passed_tests = sum(1 for outcome in self.test_outcomes if outcome.get(\"passed\", False))\n        failed_tests = total_tests - passed_tests\n        \n        summary = TestRunSummary(\n            run_id=self.run_id,\n            start_time=self.start_time,\n            end_time=current_time,\n            total_tests=total_tests,\n            passed_tests=passed_tests,\n            failed_tests=failed_tests,\n            test_outcomes=self.test_outcomes.copy(),\n        )\n        \n        summary.calculate_duration()\n        return summary\n    \n    def save_results(self, summary: Optional[TestRunSummary] = None) -> Optional[Path]:\n        \"\"\"\n        Save results to storage if storage path is configured.\n        \n        Args:\n            summary: TestRunSummary to save (current summary if not provided)\n            \n        Returns:\n            Path to saved file, None if no storage configured\n        \"\"\"\n        if not self.storage_path:\n            return None\n        \n        if summary is None:\n            summary = self.get_current_summary()\n        \n        # Create storage directory if it doesn't exist\n        self.storage_path.mkdir(parents=True, exist_ok=True)\n        \n        # Save detailed results\n        results_file = self.storage_path / f\"{self.run_id}-results.json\"\n        with open(results_file, 'w') as f:\n            json.dump({\n                \"summary\": summary.to_dict(),\n                \"metadata\": self.metadata,\n                \"raw_outcomes\": self.test_outcomes,\n            }, f, indent=2, default=str)\n        \n        # Save summary report\n        report_file = self.storage_path / f\"{self.run_id}-report.txt\"\n        with open(report_file, 'w') as f:\n            f.write(summary.get_summary_report())\n            \n            # Add detailed outcomes\n            if self.test_outcomes:\n                f.write(\"\\n\\nDetailed Test Results:\\n\")\n                f.write(\"=\" * 50 + \"\\n\")\n                for i, outcome in enumerate(self.test_outcomes, 1):\n                    status = \"PASSED\" if outcome.get(\"passed\") else \"FAILED\"\n                    f.write(f\"\\n{i}. {outcome.get('test_id', 'unknown')} - {status}\\n\")\n                    f.write(f\"   Type: {outcome.get('test_type', 'unknown')}\\n\")\n                    f.write(f\"   Duration: {outcome.get('processing_duration_ms', 0):.1f}ms\\n\")\n                    \n                    if not outcome.get(\"passed\") and outcome.get(\"error_message\"):\n                        f.write(f\"   Error: {outcome['error_message']}\\n\")\n                    \n                    if outcome.get(\"verification_details\"):\n                        f.write(f\"   Verifications: {len(outcome['verification_details'])}\\n\")\n        \n        return results_file\n    \n    def get_test_type_breakdown(self) -> Dict[str, Dict[str, int]]:\n        \"\"\"\n        Get breakdown of test results by test type.\n        \n        Returns:\n            Dictionary with test type breakdown\n        \"\"\"\n        breakdown: Dict[str, Dict[str, int]] = {}\n        \n        for outcome in self.test_outcomes:\n            test_type = outcome.get(\"test_type\", \"unknown\")\n            if test_type not in breakdown:\n                breakdown[test_type] = {\"total\": 0, \"passed\": 0, \"failed\": 0}\n            \n            breakdown[test_type][\"total\"] += 1\n            if outcome.get(\"passed\", False):\n                breakdown[test_type][\"passed\"] += 1\n            else:\n                breakdown[test_type][\"failed\"] += 1\n        \n        return breakdown\n    \n    def get_performance_stats(self) -> Dict[str, float]:\n        \"\"\"\n        Get performance statistics from test results.\n        \n        Returns:\n            Dictionary with performance statistics\n        \"\"\"\n        if not self.test_outcomes:\n            return {\"avg_duration_ms\": 0.0, \"min_duration_ms\": 0.0, \"max_duration_ms\": 0.0}\n        \n        durations = [\n            outcome.get(\"processing_duration_ms\", 0) \n            for outcome in self.test_outcomes\n        ]\n        \n        return {\n            \"avg_duration_ms\": sum(durations) / len(durations),\n            \"min_duration_ms\": min(durations),\n            \"max_duration_ms\": max(durations),\n            \"total_processing_time_ms\": sum(durations),\n        }\n    \n    def get_failure_analysis(self) -> Dict[str, Any]:\n        \"\"\"\n        Get analysis of test failures.\n        \n        Returns:\n            Dictionary with failure analysis\n        \"\"\"\n        failed_outcomes = [outcome for outcome in self.test_outcomes if not outcome.get(\"passed\", False)]\n        \n        if not failed_outcomes:\n            return {\"failure_count\": 0, \"failure_types\": {}, \"common_errors\": []}\n        \n        # Group failures by error message\n        error_counts: Dict[str, int] = {}\n        failure_types: Dict[str, int] = {}\n        \n        for outcome in failed_outcomes:\n            error_msg = outcome.get(\"error_message\", \"Unknown error\")\n            error_counts[error_msg] = error_counts.get(error_msg, 0) + 1\n            \n            test_type = outcome.get(\"test_type\", \"unknown\")\n            failure_types[test_type] = failure_types.get(test_type, 0) + 1\n        \n        # Get most common errors\n        common_errors = sorted(error_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n        \n        return {\n            \"failure_count\": len(failed_outcomes),\n            \"failure_types\": failure_types,\n            \"common_errors\": common_errors,\n            \"failure_rate_percent\": (len(failed_outcomes) / len(self.test_outcomes)) * 100,\n        }"